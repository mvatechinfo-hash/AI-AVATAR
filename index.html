<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>AI Avatar Test</title>
  <script src="https://cdn.jsdelivr.net/npm/three@0.161.0/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.161.0/examples/js/loaders/GLTFLoader.js"></script>
  <style>
    body { margin: 0; background: #000; color: white; font-family: sans-serif; }
    #chatBox { position: fixed; bottom: 10px; left: 10px; right: 10px; background: rgba(0,0,0,0.7); padding: 10px; border-radius: 10px; }
    #chatInput { width: 80%; padding: 5px; }
    #micButton { padding: 5px; cursor: pointer; background: red; border: none; border-radius: 5px; color: white; }
  </style>
</head>
<body>
  <canvas id="avatarCanvas"></canvas>

  <div id="chatBox">
    <input id="chatInput" placeholder="Type a message..." />
    <button id="micButton">ðŸŽ¤</button>
  </div>

  <script>
    const apiKey = "AIzaSyDNGCwLJ-92ULCtNX7RJKMXYO7PBhPvn_I"; // For testing only

    let scene, camera, renderer, mixer, avatar, currentAction, danceMusic;
    let actions = {};
    const animationNames = {
      idle: "idle",
      idle2: "idle2",
      wave: "wave",
      walk: "walk",
      dance: "dance"
    };

    init();
    animate();

    function init() {
      scene = new THREE.Scene();
      camera = new THREE.PerspectiveCamera(60, window.innerWidth / window.innerHeight, 0.1, 1000);
      camera.position.set(0, 1.6, 3);

      renderer = new THREE.WebGLRenderer({ canvas: document.getElementById('avatarCanvas'), antialias: true });
      renderer.setSize(window.innerWidth, window.innerHeight);

      const light = new THREE.HemisphereLight(0xffffff, 0x444444, 1);
      scene.add(light);

      const loader = new THREE.GLTFLoader();
      loader.load("your_avatar_model.glb", function(gltf) {
        avatar = gltf.scene;
        scene.add(avatar);

        console.log("Available animations:", gltf.animations.map(a => a.name));
        mixer = new THREE.AnimationMixer(avatar);
        gltf.animations.forEach(clip => {
          actions[clip.name] = mixer.clipAction(clip);
        });

        playAction(animationNames.idle);

        setTimeout(() => {
          startGreetingSequence();
        }, 1000);
      });
    }

    function playAction(name) {
      if (!actions[name]) {
        console.warn(`Animation '${name}' not found.`);
        return;
      }

      if (currentAction && currentAction.getClip().name.includes('dance') && name !== animationNames.dance) {
        stopDanceMusic();
      }

      if (currentAction && currentAction.getClip().name === name) return;

      if (currentAction) {
        currentAction.fadeOut(0.3);
      }
      currentAction = actions[name];
      currentAction.reset().fadeIn(0.3).play();
    }

    function startPatrol() {
      if (!avatar) return;
      if (!currentAction) return;
      const currentName = currentAction.getClip().name;
      if (currentName !== animationNames.idle && currentName !== animationNames.idle2) {
        return;
      }

      console.log("Patrol starting...");
      setTimeout(startPatrol, 10000);
    }

    function startGreetingSequence() {
      playAction(animationNames.wave);
      speakText("Hello! How are you? How may I help you today?");
    }

    function stopDanceMusic() {
      if (danceMusic && !danceMusic.paused) {
        danceMusic.pause();
        danceMusic.currentTime = 0;
      }
    }

    function speakText(text) {
      const utterance = new SpeechSynthesisUtterance(text);
      speechSynthesis.speak(utterance);
    }

    // Speech recognition
    const micButton = document.getElementById("micButton");
    const chatInput = document.getElementById("chatInput");
    let recognition;
    let recognitionActive = false;

    if ('webkitSpeechRecognition' in window) {
      recognition = new webkitSpeechRecognition();
      recognition.lang = "en-US";
      recognition.continuous = false;

      recognition.onresult = function(event) {
        const transcript = event.results[0][0].transcript;
        chatInput.value = transcript;
        sendMessage(transcript);
      };

      recognition.onend = function() {
        recognitionActive = false;
      };
    }

    micButton.addEventListener("click", () => {
      if (!recognition) return alert("Speech recognition not supported.");
      if (recognitionActive) {
        recognition.stop();
        return;
      }
      chatInput.value = "";
      recognition.start();
      recognitionActive = true;
    });

    function sendMessage(message) {
      fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${apiKey}`, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({
          contents: [{ role: "user", parts: [{ text: message }] }]
        })
      })
      .then(res => res.json())
      .then(data => {
        const reply = data.candidates?.[0]?.content?.parts?.[0]?.text || "I didn't understand.";
        speakText(reply);
      })
      .catch(err => console.error(err));
    }

    function animate() {
      requestAnimationFrame(animate);
      if (mixer) mixer.update(0.016);
      renderer.render(scene, camera);
    }
  </script>
</body>
</html>
